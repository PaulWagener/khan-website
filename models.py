#!/usr/bin/python
# -*- coding: utf-8 -*-
import datetime, logging
import math
import urllib
import pickle

import config_django

from google.appengine.api import users
from google.appengine.api import memcache
from google.appengine.ext import deferred
from api.jsonify import jsonify
# Do not remove this webapp.template import, as suggested
# by Guido here: http://code.google.com/p/googleappengine/issues/detail?id=3632
from google.appengine.ext.webapp import template
from django.template.defaultfilters import slugify

from google.appengine.ext import db
import object_property
import app
import util
import consts
import points
from search import Searchable
from app import App
import layer_cache
import request_cache
from discussion import models_discussion
from topics_list import all_topics_list
import nicknames
from counters import user_counter
from facebook_util import is_facebook_user_id

# Setting stores per-application key-value pairs
# for app-wide settings that must be synchronized
# across all GAE instances.

class Setting(db.Model):

    value = db.StringProperty()

    @staticmethod
    def entity_group_key():
        return db.Key.from_path('Settings', 'default_settings')

    @staticmethod
    def _get_or_set_with_key(key, val = None):
        if val is None:
            return Setting._cache_get_by_key_name(key)
        else:
            setting = Setting(Setting.entity_group_key(), key, value=str(val))
            setting_old = Setting(key_name=key, value=str(val)) # delete once migration complete
            db.put([setting, setting_old])
            Setting._get_settings_dict(bust_cache=True)
            return setting.value

    @staticmethod
    def _cache_get_by_key_name(key):
        setting = Setting._get_settings_dict().get(key)
        if setting is not None:
            return setting.value
        return None

    @staticmethod
    @request_cache.cache()
    @layer_cache.cache(layer=layer_cache.Layers.Memcache)
    def _get_settings_dict(bust_cache = False):
        # ancestor query to ensure consistent results
        query = Setting.all().ancestor(Setting.entity_group_key())
        results = dict((setting.key().name(), setting) for setting in query.fetch(20))

        # backfill with old style settings
        for setting in Setting.all().fetch(20):
            key = setting.key()
            if key.parent() is None and not key.name() in results.keys():
                results[key.name()] = setting

        return results

    @staticmethod
    def cached_library_content_date(val = None):
        return Setting._get_or_set_with_key("cached_library_content_date", val)

    @staticmethod
    def cached_exercises_date(val = None):
        return Setting._get_or_set_with_key("cached_exercises_date", val)

    @staticmethod
    def count_videos(val = None):
        return Setting._get_or_set_with_key("count_videos", val) or 0

    @staticmethod
    def last_youtube_sync_generation_start(val = None):
        return Setting._get_or_set_with_key("last_youtube_sync_generation_start", val) or 0

class Exercise(db.Model):

    name = db.StringProperty()
    short_display_name = db.StringProperty(default="")
    prerequisites = db.StringListProperty()
    covers = db.StringListProperty()
    v_position = db.IntegerProperty()
    h_position = db.IntegerProperty()
    seconds_per_fast_problem = db.FloatProperty(default = consts.MIN_SECONDS_PER_FAST_PROBLEM) # Seconds expected to finish a problem 'quickly' for badge calculation

    # True if this exercise is live and visible to all users.
    # Non-live exercises are only visible to admins.
    live = db.BooleanProperty(default=False)

    # True if this exercise is a quasi-exercise generated by
    # combining the content of other exercises
    summative = db.BooleanProperty(default=False)

    # Teachers contribute raw html with embedded CSS and JS
    # and we sanitize it with Caja before displaying it to
    # students.
    author = db.UserProperty()
    raw_html = db.TextProperty()
    last_modified = db.DateTimeProperty()
    safe_html = db.TextProperty()
    safe_js = db.TextProperty()
    last_sanitized = db.DateTimeProperty(default=datetime.datetime.min)
    sanitizer_used = db.StringProperty()

    _serialize_blacklist = [
            "author", "raw_html", "last_modified", "safe_html", "safe_js",
            "last_sanitized", "sanitizer_used", "coverers", "prerequisites_ex", "assigned",
            ]

    @property
    def ka_url(self):
        return absolute_url("/exercises?exid=%s" % self.name)

    @staticmethod
    def get_by_name(name):
        dict_exercises = Exercise.__get_dict_use_cache_unsafe__()
        if dict_exercises.has_key(name):
            if dict_exercises[name].is_visible_to_current_user():
                return dict_exercises[name]
        return None

    @staticmethod
    def to_display_name(name):
        if name:
            return name.replace('_', ' ').capitalize()
        return ""

    @property
    def display_name(self):
        return Exercise.to_display_name(self.name)

    @property
    def required_streak(self):
        if self.summative:
            return consts.REQUIRED_STREAK * len(self.prerequisites)
        else:
            return consts.REQUIRED_STREAK

    @staticmethod
    def to_short_name(name):
        exercise = Exercise.get_by_name(name)
        if exercise:
            return exercise.short_name()
        return ""

    def short_name(self):
        if self.short_display_name:
            return self.short_display_name[:11]
        return self.display_name[:11]

    def is_visible_to_current_user(self):
        return self.live or users.is_current_user_admin()

    def struggling_threshold(self):
        return 3 * self.required_streak

    def summative_children(self):
        if not self.summative:
            return []
        query = db.Query(Exercise)
        query.filter("name IN ", self.prerequisites)
        return query

    def non_summative_exercise(self, problem_number):
        if not self.summative:
            return self

        if len(self.prerequisites) <= 0:
            raise Exception("Summative exercise '%s' does not include any other exercises" % self.name)

        # For now we just cycle through all of the included exercises in a summative exercise
        index = int(problem_number) % len(self.prerequisites)
        exid = self.prerequisites[index]

        query = Exercise.all()
        query.filter('name =', exid)
        exercise = query.get()

        if not exercise:
            raise Exception("Unable to find included exercise")

        if exercise.summative:
            return exercise.non_summative_exercise(problem_number)
        else:
            return exercise

    def related_videos_query(self):
        exercise_videos = None
        query = ExerciseVideo.all()
        query.filter('exercise =', self.key()).order('exercise_order')
        return query

    @layer_cache.cache_with_key_fxn(lambda self: "related_videos_%s" % self.key(), layer=layer_cache.Layers.Memcache)
    def related_videos_fetch(self):
        exercise_videos = self.related_videos_query().fetch(10)
        for exercise_video in exercise_videos:
            exercise_video.video # Pre-cache video entity
        return exercise_videos

    @classmethod
    def all(cls, live_only = False):
        query = super(Exercise, cls).all()
        if live_only or not users.is_current_user_admin():
            query.filter("live =", True)
        return query

    @classmethod
    def all_unsafe(cls):
        return super(Exercise, cls).all()

    @staticmethod
    def get_all_use_cache():
        if users.is_current_user_admin():
            return Exercise.__get_all_use_cache_unsafe__()
        else:
            return Exercise.__get_all_use_cache_safe__()

    @staticmethod
    @layer_cache.cache_with_key_fxn(lambda *args, **kwargs: "all_exercises_unsafe_%s" % Setting.cached_exercises_date())
    def __get_all_use_cache_unsafe__():
        query = Exercise.all_unsafe().order('h_position')
        return query.fetch(200)

    @staticmethod
    @layer_cache.cache_with_key_fxn(lambda *args, **kwargs: "all_exercises_safe_%s" % Setting.cached_exercises_date())
    def __get_all_use_cache_safe__():
        query = Exercise.all(live_only = True).order('h_position')
        return query.fetch(200)

    @staticmethod
    @layer_cache.cache_with_key_fxn(lambda *args, **kwargs: "all_exercises_dict_unsafe_%s" % Setting.cached_exercises_date())
    def __get_dict_use_cache_unsafe__():
        exercises = Exercise.__get_all_use_cache_unsafe__()
        dict_exercises = {}
        for exercise in exercises:
            dict_exercises[exercise.name] = exercise
        return dict_exercises

    _EXERCISES_COUNT_KEY = "Exercise.count()"
    @staticmethod
    def get_count():
        count = memcache.get(Exercise._EXERCISES_COUNT_KEY, namespace=App.version)
        if count is None:
            count = Exercise.all().count()
            memcache.set(Exercise._EXERCISES_COUNT_KEY, count, namespace=App.version)
        return count

    def put(self):
        Setting.cached_exercises_date(str(datetime.datetime.now()))
        memcache.delete(Exercise._EXERCISES_COUNT_KEY, namespace=App.version)
        db.Model.put(self)

    @staticmethod
    def get_dict(query, fxn_key):
        exercise_dict = {}
        for exercise in query.fetch(10000):
            exercise_dict[fxn_key(exercise)] = exercise
        return exercise_dict

class UserExercise(db.Model):

    user = db.UserProperty()
    exercise = db.StringProperty()
    exercise_model = db.ReferenceProperty(Exercise)
    streak = db.IntegerProperty(default = 0)
    longest_streak = db.IntegerProperty(default = 0)
    first_done = db.DateTimeProperty(auto_now_add=True)
    last_done = db.DateTimeProperty()
    total_done = db.IntegerProperty(default = 0)
    total_correct = db.IntegerProperty(default = 0)
    last_review = db.DateTimeProperty(default=datetime.datetime.min)
    review_interval_secs = db.IntegerProperty(default=(60 * 60 * 24 * consts.DEFAULT_REVIEW_INTERVAL_DAYS)) # Default 7 days until review
    proficient_date = db.DateTimeProperty()
    seconds_per_fast_problem = db.FloatProperty(default = consts.MIN_SECONDS_PER_FAST_PROBLEM) # Seconds expected to finish a problem 'quickly' for badge calculation
    summative = db.BooleanProperty(default=False)

    _USER_EXERCISE_KEY_FORMAT = "UserExercise.all().filter('user = '%s')"

    _serialize_blacklist = ["review_interval_secs"]

    @property
    def required_streak(self):
        if self.summative:
            return self.exercise_model.required_streak
        else:
            return consts.REQUIRED_STREAK

    @property
    def exercise_states(self):
        user_data = self.get_user_data()
        if user_data:
            return user_data.get_exercise_states(self.exercise_model, self)
        return None

    @property
    def next_points(self):
        user_data = self.get_user_data()

        suggested = proficient = False

        if user_data:
            suggested = user_data.is_suggested(self.exercise)
            proficient = user_data.is_proficient_at(self.exercise)

        return points.ExercisePointCalculator(self, suggested, proficient)

    @staticmethod
    def get_key_for_email(email):
        return UserExercise._USER_EXERCISE_KEY_FORMAT % email

    @staticmethod
    def get_for_user_data(user_data):
        query = UserExercise.all()
        query.filter('user =', user_data.user)
        return query

    @staticmethod
    @request_cache.cache_with_key_fxn(lambda user_data: "request_cache_user_exercise_%s" % user_data.key_email)
    def get_for_user_data_use_cache(user_data):
        user_exercises_key = UserExercise.get_key_for_email(user_data.key_email)
        user_exercises = memcache.get(user_exercises_key, namespace=App.version)
        if user_exercises is None:
            user_exercises = UserExercise.get_for_user_data(user_data).fetch(1000)
            memcache.set(user_exercises_key, user_exercises, namespace=App.version)
        return user_exercises

    def get_user_data(self):
        user_data = None

        if hasattr(self, "_user_data"):
            user_data = self._user_data
        else:
            user_data = UserData.get_from_db_key_email(self.user.email())

        if not user_data:
            logging.critical("Empty user data for UserExercise w/ .user = %s" % self.user)

        return user_data

    def clear_memcache(self):
        memcache.delete(UserExercise.get_key_for_email(self.user.email()), namespace=App.version)

    def put(self):
        self.clear_memcache()
        db.Model.put(self)

    def belongs_to(self, user_data):
        return user_data and self.user.email().lower() == user_data.key_email.lower()

    def reset_streak(self):
        if self.exercise_model.summative:
            # Reset streak to latest 10 milestone
            self.streak = (self.streak / consts.CHALLENGE_STREAK_BARRIER) * consts.CHALLENGE_STREAK_BARRIER
        else:
            self.streak = 0

    def struggling_threshold(self):
        return self.exercise_model.struggling_threshold()

    @staticmethod
    def is_struggling_with(user_exercise, exercise):
        return user_exercise.streak == 0 and user_exercise.longest_streak < exercise.required_streak and user_exercise.total_done > exercise.struggling_threshold()

    def is_struggling(self):
        return UserExercise.is_struggling_with(self, self.exercise_model)

    def get_review_interval(self):
        review_interval = datetime.timedelta(seconds=self.review_interval_secs)

        if review_interval.days < consts.MIN_REVIEW_INTERVAL_DAYS:
            review_interval = datetime.timedelta(days=consts.MIN_REVIEW_INTERVAL_DAYS)
        elif review_interval.days > consts.MAX_REVIEW_INTERVAL_DAYS:
            review_interval = datetime.timedelta(days=consts.MAX_REVIEW_INTERVAL_DAYS)

        return review_interval

    def schedule_review(self, correct, now=datetime.datetime.now()):
        # If the user is not now and never has been proficient, don't schedule a review
        if (self.streak + correct) < self.required_streak and self.longest_streak < self.required_streak:
            return

        # If the user is hitting a new streak either for the first time or after having lost
        # proficiency, reset their review interval counter.
        if (self.streak + correct) >= self.required_streak:
            self.review_interval_secs = 60 * 60 * 24 * consts.DEFAULT_REVIEW_INTERVAL_DAYS

        review_interval = self.get_review_interval()

        if correct and self.last_review != datetime.datetime.min:
            time_since_last_review = now - self.last_review
            if time_since_last_review >= review_interval:
                review_interval = time_since_last_review * 2
        if not correct:
            review_interval = review_interval // 2
        if correct:
            self.last_review = now
        else:
            self.last_review = datetime.datetime.min
        self.review_interval_secs = review_interval.days * 86400 + review_interval.seconds

    def set_proficient(self, proficient, user_data):
        if not proficient and self.longest_streak < self.required_streak:
            # Not proficient and never has been so nothing to do
            return

        if proficient:
            if self.exercise not in user_data.proficient_exercises:
                self.proficient_date = datetime.datetime.now()

                user_data.proficient_exercises.append(self.exercise)
                user_data.need_to_reassess = True
                user_data.put()

                util_notify.update(user_data, self, False, True)

        else:
            if self.exercise in user_data.proficient_exercises:
                user_data.proficient_exercises.remove(self.exercise)
                user_data.need_to_reassess = True
                user_data.put()

class CoachRequest(db.Model):
    coach_requesting = db.UserProperty()
    student_requested = db.UserProperty()

    @property
    def coach_requesting_data(self):
        if not hasattr(self, "coach_user_data"):
            self.coach_user_data = UserData.get_from_db_key_email(self.coach_requesting.email())
        return self.coach_user_data

    @property
    def student_requested_data(self):
        if not hasattr(self, "student_user_data"):
            self.student_user_data = UserData.get_from_db_key_email(self.student_requested.email())
        return self.student_user_data

    @staticmethod
    def key_for(user_data_coach, user_data_student):
        return "%s_request_for_%s" % (user_data_coach.key_email, user_data_student.key_email)

    @staticmethod
    def get_for(user_data_coach, user_data_student):
        return CoachRequest.get_by_key_name(CoachRequest.key_for(user_data_coach, user_data_student))

    @staticmethod
    def get_or_insert_for(user_data_coach, user_data_student):
        return CoachRequest.get_or_insert(
                key_name = CoachRequest.key_for(user_data_coach, user_data_student),
                coach_requesting = user_data_coach.user,
                student_requested = user_data_student.user,
                )

    @staticmethod
    def get_for_student(user_data_student):
        return CoachRequest.all().filter("student_requested = ", user_data_student.user)

    @staticmethod
    def get_for_coach(user_data_coach):
        return CoachRequest.all().filter("coach_requesting = ", user_data_coach.user)

class StudentList(db.Model):
    name = db.StringProperty()
    coaches = db.ListProperty(db.Key)

    def delete(self, *args, **kwargs):
        self.remove_all_students()
        db.Model.delete(self, *args, **kwargs)

    def remove_all_students(self):
        students = self.get_students_data()
        for s in students:
            s.student_lists.remove(self.key())
        db.put(students)

    @property
    def students(self):
        return UserData.all().filter("student_lists = ", self.key())

    # these methods have the same interface as the methods on UserData
    def get_students_data(self):
        return [s for s in self.students]

    @staticmethod
    def get_for_coach(key):
        query = StudentList.all()
        query.filter("coaches = ", key)
        return query

class UserVideoCss(db.Model):
    user = db.UserProperty()
    video_css = db.TextProperty()
    pickled_dict = db.BlobProperty()
    last_modified = db.DateTimeProperty(required=True, auto_now=True)
    version = db.IntegerProperty(default=0)

    STARTED, COMPLETED = range(2)

    @staticmethod
    def get_for_user_data(user_data):
        p = pickle.dumps({'started': set([]), 'completed': set([])})
        return UserVideoCss.get_or_insert(UserVideoCss._key_for(user_data),
                                          user=user_data.user,
                                          video_css='',
                                          pickled_dict=p,
                                          )

    @staticmethod
    def _key_for(user_data):
        return 'user_video_css_%s' % user_data.key_email

    @staticmethod
    def set_started(user_data, video, version):
        deferred.defer(set_css_deferred, user_data.key(), video.key(), UserVideoCss.STARTED, version)

    @staticmethod
    def set_completed(user_data, video, version):
        deferred.defer(set_css_deferred, user_data.key(), video.key(), UserVideoCss.COMPLETED, version)

    @staticmethod
    def _chunker(seq, size):
        return (seq[pos:pos + size] for pos in xrange(0, len(seq), size))

    def load_pickled(self):
        max_selectors = 20
        css_list = []
        css = pickle.loads(self.pickled_dict)

        started_css = '{background-image:url(/images/video-indicator-started.png);padding-left:14px;}'
        complete_css = '{background-image:url(/images/video-indicator-complete.png);padding-left:14px;}'

        for id in UserVideoCss._chunker(list(css['started']), max_selectors):
            css_list.append(','.join(id))
            css_list.append(started_css)

        for id in UserVideoCss._chunker(list(css['completed']), max_selectors):
            css_list.append(','.join(id))
            css_list.append(complete_css)

        self.video_css = ''.join(css_list)

def set_css_deferred(user_data_key, video_key, status, version):
    user_data = UserData.get(user_data_key)
    uvc = UserVideoCss.get_for_user_data(user_data)
    css = pickle.loads(uvc.pickled_dict)

    id = '.v%d' % video_key.id()
    if status == UserVideoCss.STARTED:
        css['completed'].discard(id)
        css['started'].add(id)
    else:
        css['started'].discard(id)
        css['completed'].add(id)

    uvc.pickled_dict = pickle.dumps(css)
    uvc.load_pickled()
    uvc.version = version
    db.put(uvc)

class UserData(db.Model):
    user = db.UserProperty()
    user_id = db.StringProperty()
    user_nickname = db.StringProperty()
    current_user = db.UserProperty()
    moderator = db.BooleanProperty(default=False)
    developer = db.BooleanProperty(default=False)
    joined = db.DateTimeProperty(auto_now_add=True)
    last_login = db.DateTimeProperty()
    proficient_exercises = db.StringListProperty() # Names of exercises in which the user is *explicitly* proficient
    all_proficient_exercises = db.StringListProperty() # Names of all exercises in which the user is proficient
    suggested_exercises = db.StringListProperty()
    assigned_exercises = db.StringListProperty()
    badges = db.StringListProperty() # All awarded badges
    need_to_reassess = db.BooleanProperty()
    points = db.IntegerProperty(default = 0)
    total_seconds_watched = db.IntegerProperty(default = 0)
    coaches = db.StringListProperty()
    student_lists = db.ListProperty(db.Key)
    map_coords = db.StringProperty()
    expanded_all_exercises = db.BooleanProperty(default=True)
    videos_completed = db.IntegerProperty(default = -1)
    last_daily_summary = db.DateTimeProperty()
    last_badge_review = db.DateTimeProperty()
    last_activity = db.DateTimeProperty()
    count_feedback_notification = db.IntegerProperty(default = -1)
    question_sort_order = db.IntegerProperty(default = -1)
    user_email = db.StringProperty()
    uservideocss_version = db.IntegerProperty(default = 0)

    _serialize_blacklist = [
            "assigned_exercises", "badges", "count_feedback_notification",
            "last_daily_summary", "need_to_reassess", "videos_completed",
            "moderator", "expanded_all_exercises", "question_sort_order",
            "last_login", "user", "current_user", "map_coords", "expanded_all_exercises",
            "user_nickname", "user_email",
    ]

    @property
    def nickname(self):
        # Only return cached value if it exists and it wasn't cached during a Facebook API hiccup
        if self.user_nickname and not is_facebook_user_id(self.user_nickname):
            return self.user_nickname
        else:
            return nicknames.get_nickname_for(self)

    @property
    def email(self):
        if self.user_email:
            return self.user_email
        else:
            return self.current_user.email()

    @property
    def key_email(self):
        return self.user.email()

    @property
    def badge_counts(self):
        return util_badges.get_badge_counts(self)

    @staticmethod
    @request_cache.cache()
    def current(bust_cache=True):
        if bust_cache:
            util.get_current_user_id(bust_cache=True)

        user_id = util.get_current_user_id()
        email = user_id

        google_user = users.get_current_user()
        if google_user:
            email = google_user.email()

        if user_id:
            # Once we have rekeyed legacy entities,
            # we will be able to simplify this.we make
            return  UserData.get_from_user_id(user_id) or \
                    UserData.get_from_db_key_email(email) or \
                    UserData.insert_for(user_id, email)
        return None

    @staticmethod
    def pre_phantom():
        pre_phantom_email = "http://nouserid.khanacademy.org/pre-phantom-user-2"
        return UserData.insert_for(pre_phantom_email, pre_phantom_email)

    @property
    def is_phantom(self):
        return util.is_phantom_user(self.user_id)

    @staticmethod
    @request_cache.cache_with_key_fxn(lambda user_id: "UserData_user_id:%s" % user_id)
    def get_from_user_id(user_id):
        if not user_id:
            return None

        query = UserData.all()
        query.filter('user_id =', user_id)
        query.order('-points') # Temporary workaround for issue 289

        return query.get()

    @staticmethod
    def get_from_user_input_email(email):
        if not email:
            return None

        query = UserData.all()
        query.filter('user_email =', email)
        query.order('-points') # Temporary workaround for issue 289

        return query.get()

    @staticmethod
    def get_from_db_key_email(email):
        if not email:
            return None

        query = UserData.all()
        query.filter('user =', users.User(email))
        query.order('-points') # Temporary workaround for issue 289

        return query.get()

    @staticmethod
    def insert_for(user_id, email):
        if not user_id or not email:
            return None

        user = users.User(email)
        key = "user_id_key_%s" % user_id

        user_data = UserData.get_or_insert(
            key_name=key,
            user=user,
            current_user=user,
            user_id=user_id,
            moderator=False,
            last_login=datetime.datetime.now(),
            proficient_exercises=[],
            suggested_exercises=[],
            assigned_exercises=[],
            need_to_reassess=True,
            points=0,
            coaches=[],
            user_email=email

            )

        if not user_data.is_phantom:
            # Record that we now have one more registered user
            if (datetime.datetime.now() - user_data.joined).seconds < 60:
                # Extra safety check against user_data.joined in case some
                # subtle bug results in lots of calls to insert_for for
                # UserData objects with existing key_names.
                user_counter.add(1)

        return user_data

    def delete(self):
        logging.info("Deleting user data for %s with points %s" % (self.key_email, self.points))
        logging.info("Dumping user data for %s: %s" % (self.user_id, jsonify(self)))

        if not self.is_phantom:
            user_counter.add(-1)

        db.delete(self)

    def get_or_insert_exercise(self, exercise, allow_insert = True):
        if not exercise:
            return None

        exid = exercise.name
        userExercise = UserExercise.get_by_key_name(exid, parent=self)

        if not userExercise:
            # There are some old entities lying around that don't have keys.
            # We have to check for them here, but once we have reparented and rekeyed legacy entities,
            # this entire function can just be a call to .get_or_insert()
            query = UserExercise.all(keys_only = True)
            query.filter('user =', self.user)
            query.filter('exercise =', exid)
            query.order('-total_done') # Temporary workaround for issue 289

            # In order to guarantee consistency in the HR datastore, we need to query
            # via db.get for these old, parent-less entities.
            key_user_exercise = query.get()
            if key_user_exercise:
                userExercise = UserExercise.get(str(key_user_exercise))

        if allow_insert and not userExercise:
            userExercise = UserExercise.get_or_insert(
                key_name=exid,
                parent=self,
                user=self.user,
                exercise=exid,
                exercise_model=exercise,
                streak=0,
                longest_streak=0,
                first_done=datetime.datetime.now(),
                last_done=None,
                total_done=0,
                summative=exercise.summative,
                )

        return userExercise

    def get_exercise_states(self, exercise, user_exercise, current_time = datetime.datetime.now()):
        phantom = exercise.phantom = util.is_phantom_user(self.user_id)
        proficient = exercise.proficient = self.is_proficient_at(exercise.name)
        suggested = exercise.suggested = self.is_suggested(exercise.name)
        reviewing = exercise.review = self.is_reviewing(exercise.name, user_exercise, current_time)
        struggling = UserExercise.is_struggling_with(user_exercise, exercise)
        endangered = proficient and user_exercise.streak == 0 and user_exercise.longest_streak >= exercise.required_streak

        return {
            'phantom': phantom,
            'proficient': proficient,
            'suggested': suggested,
            'reviewing': reviewing,
            'struggling': struggling,
            'endangered': endangered,
            'summative': exercise.summative,
        }

    def reassess_from_graph(self, ex_graph):
        all_proficient_exercises = []
        for ex in ex_graph.get_proficient_exercises():
            all_proficient_exercises.append(ex.name)
        suggested_exercises = []
        for ex in ex_graph.get_suggested_exercises():
            suggested_exercises.append(ex.name)
        is_changed = (all_proficient_exercises != self.all_proficient_exercises or
                      suggested_exercises != self.suggested_exercises)
        self.all_proficient_exercises = all_proficient_exercises
        self.suggested_exercises = suggested_exercises
        self.need_to_reassess = False
        return is_changed

    def reassess_if_necessary(self):
        if not self.need_to_reassess or self.all_proficient_exercises is None:
            return False
        ex_graph = ExerciseGraph(self)
        return self.reassess_from_graph(ex_graph)

    def is_proficient_at(self, exid):
        self.reassess_if_necessary()
        return (exid in self.all_proficient_exercises)

    def is_explicitly_proficient_at(self, exid):
        return (exid in self.proficient_exercises)

    def is_reviewing(self, exid, user_exercise, time):

        # Short circuit out of full review check if not proficient or review time hasn't come around yet

        if not self.is_proficient_at(exid):
            return False

        if user_exercise.last_review + user_exercise.get_review_interval() > time:
            return False

        ex_graph = ExerciseGraph(self)
        review_exercise_names = map(lambda exercise: exercise.name, ex_graph.get_review_exercises(time))
        return (exid in review_exercise_names)

    def is_suggested(self, exid):
        self.reassess_if_necessary()
        return (exid in self.suggested_exercises)

    def get_students_data(self):
        coach_email = self.key_email
        query = UserData.all().filter('coaches =', coach_email)
        students_data = [s for s in query.fetch(1000)]

        if coach_email.lower() != coach_email:
            students_set = set([s.key().id_or_name() for s in students_data])
            query = UserData.all().filter('coaches =', coach_email.lower())
            for student_data in query:
                if student_data.key().id_or_name() not in students_set:
                    students_data.append(student_data)
        return students_data

    def has_students(self):
        return len(self.get_students_data()) > 0

    def coach_emails(self):
        emails = []
        for key_email in self.coaches:
            user_data_coach = UserData.get_from_db_key_email(key_email)
            if user_data_coach:
                emails.append(user_data_coach.email)
        return emails

    def is_coached_by(self, user_data_coach):
        return user_data_coach.key_email in self.coaches or user_data_coach.key_email.lower() in self.coaches

    def add_points(self, points):
        if self.points == None:
            self.points = 0
        if (self.points % 2500) > ((self.points+points) % 2500): #Check if we crossed an interval of 2500 points
            util_notify.update(self,None,True)
        self.points += points


    def get_videos_completed(self):
        if self.videos_completed < 0:
            self.videos_completed = UserVideo.count_completed_for_user_data(self)
            self.put()
        return self.videos_completed

    def feedback_notification_count(self):
        if self.count_feedback_notification == -1:
            self.count_feedback_notification = models_discussion.FeedbackNotification.gql("WHERE user = :1", self.user).count()
            self.put()
        return self.count_feedback_notification

class Video(Searchable, db.Model):
    youtube_id = db.StringProperty()
    url = db.StringProperty()
    title = db.StringProperty()
    description = db.TextProperty()
    playlists = db.StringListProperty()
    keywords = db.StringProperty()
    duration = db.IntegerProperty(default = 0)

    # Human readable, unique id that can be used in URLS.
    readable_id = db.StringProperty()

    # YouTube view count from last sync.
    views = db.IntegerProperty(default = 0)

    # Date first added via KA library sync with YouTube.
    # This property hasn't always existsed, so for many old videos
    # this date may be much later than the actual YouTube upload date.
    date_added = db.DateTimeProperty(auto_now_add=True)

    # Last download version in which video download was prepped.
    download_version = db.IntegerProperty(default = 0)
    CURRENT_DOWNLOAD_VERSION = 2

    _serialize_blacklist = ["download_version", "CURRENT_DOWNLOAD_VERSION"]

    INDEX_ONLY = ['title', 'keywords', 'description']
    INDEX_TITLE_FROM_PROP = 'title'
    INDEX_USES_MULTI_ENTITIES = False

    @property
    def ka_url(self):
      return util.absolute_url('/video/%s' % self.readable_id)

    @property
    def download_urls(self):
        if self.download_version == Video.CURRENT_DOWNLOAD_VERSION:
            download_url_base = "http://www.archive.org/download/KA-converted-%s" % self.youtube_id

            return {
                    "mp4": "%s/%s.mp4" % (download_url_base, self.youtube_id),
                    "png": "%s/%s.png" % (download_url_base, self.youtube_id),
                    }

        return None

    def download_video_url(self):
        download_urls = self.download_urls
        if download_urls:
            return download_urls.get("mp4")
        return None

    @staticmethod
    def get_for_readable_id(readable_id):
        video = None
        query = Video.all()
        query.filter('readable_id =', readable_id)
        # The following should just be:
        # video = query.get()
        # but the database currently contains multiple Video objects for a particular
        # video.  Some are old.  Some are due to a YouTube sync where the youtube urls
        # changed and our code was producing youtube_ids that ended with '_player'.
        # This hack gets the most recent valid Video object.
        key_id = 0
        for v in query:
            if v.key().id() > key_id and not v.youtube_id.endswith('_player'):
                video = v
                key_id = v.key().id()
        # End of hack
        return video

    def first_playlist(self):
        query = VideoPlaylist.all()
        query.filter('video =', self)
        query.filter('live_association =', True)
        video_playlist = query.get()
        if video_playlist:
            return video_playlist.playlist
        return None

    def current_user_points(self):
        user_video = UserVideo.get_for_video_and_user_data(self, UserData.current())
        if user_video:
            return points.VideoPointCalculator(user_video)
        else:
            return 0

    @staticmethod
    def get_dict(query, fxn_key):
        video_dict = {}
        for video in query.fetch(10000):
            video_dict[fxn_key(video)] = video
        return video_dict

    def related_exercises(self):
        exercise_videos = None
        query = ExerciseVideo.all()
        query.filter('video =', self.key())
        return query

    @layer_cache.cache_with_key_fxn(lambda self: "related_exercise_%s" % self.key(), layer=layer_cache.Layers.Memcache)
    def get_related_exercise(self):
        exercise_video = self.related_exercises().get()
        if exercise_video:
            exercise_video.exercise # Pre-cache exercise entity
        return exercise_video or ExerciseVideo()

class Playlist(Searchable, db.Model):

    youtube_id = db.StringProperty()
    url = db.StringProperty()
    title = db.StringProperty()
    description = db.TextProperty()
    readable_id = db.StringProperty() #human readable, but unique id that can be used in URLS
    INDEX_ONLY = ['title', 'description']
    INDEX_TITLE_FROM_PROP = 'title'
    INDEX_USES_MULTI_ENTITIES = False

    _serialize_blacklist = ["readable_id"]

    @property
    def ka_url(self):
        return util.absolute_url('#%s' % urllib.quote(slugify(self.title)))

    @staticmethod
    def get_for_all_topics():
        playlists = []
        for playlist in Playlist.all().fetch(1000):
            if playlist.title in all_topics_list:
                playlists.append(playlist)
        return playlists

class UserPlaylist(db.Model):
    user = db.UserProperty()
    playlist = db.ReferenceProperty(Playlist)
    seconds_watched = db.IntegerProperty(default = 0)
    last_watched = db.DateTimeProperty(auto_now_add = True)
    title = db.StringProperty()

    @staticmethod
    def get_for_user_data(user_data):
        query = UserPlaylist.all()
        query.filter('user =', user_data.user)
        return query

    @staticmethod
    def get_key_name(playlist, user_data):
        return user_data.key_email + ":" + playlist.youtube_id

    @staticmethod
    def get_for_playlist_and_user_data(playlist, user_data, insert_if_missing=False):
        if not user_data:
            return None

        key = UserPlaylist.get_key_name(playlist, user_data)

        if insert_if_missing:
            return UserPlaylist.get_or_insert(
                        key_name = key,
                        user = user_data.user,
                        playlist = playlist)
        else:
            return UserPlaylist.get_by_key_name(key)

class UserVideo(db.Model):

    @staticmethod
    def get_key_name(video, user_data):
        return user_data.key_email + ":" + video.youtube_id

    @staticmethod
    def get_for_video_and_user_data(video, user_data, insert_if_missing=False):
        if not user_data:
            return None
        key = UserVideo.get_key_name(video, user_data)

        if insert_if_missing:
            return UserVideo.get_or_insert(
                        key_name = key,
                        user = user_data.user,
                        video = video,
                        duration = video.duration)
        else:
            return UserVideo.get_by_key_name(key)

    @staticmethod
    def count_completed_for_user_data(user_data):
        query = UserVideo.all()
        query.filter("user = ", user_data.user)
        query.filter("completed = ", True)
        return query.count(limit=10000)

    user = db.UserProperty()
    video = db.ReferenceProperty(Video)

    # Farthest second in video watched
    last_second_watched = db.IntegerProperty(default = 0)

    # Number of seconds actually spent watching this video, regardless of jumping around to various
    # scrubber positions. This value can exceed the total duration of the video if it is watched
    # many times, and it doesn't necessarily match the percent watched.
    seconds_watched = db.IntegerProperty(default = 0)

    last_watched = db.DateTimeProperty(auto_now_add = True)
    duration = db.IntegerProperty(default = 0)
    completed = db.BooleanProperty(default = False)

    @property
    def points(self):
        return points.VideoPointCalculator(self)

class VideoLog(db.Model):
    user = db.UserProperty()
    video = db.ReferenceProperty(Video)
    video_title = db.StringProperty()
    time_watched = db.DateTimeProperty(auto_now_add = True)
    seconds_watched = db.IntegerProperty(default = 0)
    last_second_watched = db.IntegerProperty()
    points_earned = db.IntegerProperty(default = 0)
    playlist_titles = db.StringListProperty()

    _serialize_blacklist = ["video"]

    @staticmethod
    def get_for_user_data_between_dts(user_data, dt_a, dt_b):
        query = VideoLog.all()
        query.filter('user =', user_data.user)

        query.filter('time_watched >=', dt_a)
        query.filter('time_watched <=', dt_b)
        query.order('time_watched')

        return query

    @staticmethod
    def get_for_user_data_and_video(user_data, video):
        query = VideoLog.all()

        query.filter('user =', user_data.user)
        query.filter('video =', video)

        query.order('time_watched')

        return query

    @staticmethod
    def add_entry(user_data, video, seconds_watched, last_second_watched):

        user_video = UserVideo.get_for_video_and_user_data(video, user_data, insert_if_missing=True)

        # Cap seconds_watched at duration of video
        seconds_watched = max(0, min(seconds_watched, video.duration))

        video_points_previous = points.VideoPointCalculator(user_video)

        action_cache=last_action_cache.LastActionCache.get_for_user_data(user_data)

        last_video_log = action_cache.get_last_video_log()

        # If the last video logged is not this video and the times being credited
        # overlap, don't give points for this video. Can only get points for one video
        # at a time.
        if last_video_log and last_video_log.key_for_video() != video.key():
            dt_now = datetime.datetime.now()
            if last_video_log.time_watched > (dt_now - datetime.timedelta(seconds=seconds_watched)):
                return (None, None, 0)

        video_log = VideoLog()
        video_log.user = user_data.user
        video_log.video = video
        video_log.video_title = video.title
        video_log.seconds_watched = seconds_watched
        video_log.last_second_watched = last_second_watched

        if last_second_watched > user_video.last_second_watched:
            user_video.last_second_watched = last_second_watched

        if seconds_watched > 0:
            if user_video.seconds_watched == 0:
                user_data.uservideocss_version += 1
                UserVideoCss.set_started(user_data, user_video.video, user_data.uservideocss_version)

            user_video.seconds_watched += seconds_watched
            user_data.total_seconds_watched += seconds_watched

            # Update seconds_watched of all associated UserPlaylists
            query = VideoPlaylist.all()
            query.filter('video =', video)
            query.filter('live_association = ', True)

            first_video_playlist = True
            for video_playlist in query:
                user_playlist = UserPlaylist.get_for_playlist_and_user_data(video_playlist.playlist, user_data, insert_if_missing=True)
                user_playlist.title = video_playlist.playlist.title
                user_playlist.seconds_watched += seconds_watched
                user_playlist.last_watched = datetime.datetime.now()
                user_playlist.put()

                video_log.playlist_titles.append(user_playlist.title)

                if first_video_playlist:
                    action_cache.push_video_log(video_log)

                util_badges.update_with_user_playlist(
                        user_data,
                        user_playlist,
                        include_other_badges = first_video_playlist,
                        action_cache = action_cache)

                first_video_playlist = False

        user_video.last_watched = datetime.datetime.now()
        user_video.duration = video.duration

        user_data.last_activity = user_video.last_watched

        video_points_total = points.VideoPointCalculator(user_video)
        video_points_received = video_points_total - video_points_previous

        if not user_video.completed and video_points_total >= consts.VIDEO_POINTS_BASE:
            # Just finished this video for the first time
            user_video.completed = True
            user_data.videos_completed = -1

            user_data.uservideocss_version += 1
            UserVideoCss.set_completed(user_data, user_video.video, user_data.uservideocss_version)

        if video_points_received > 0:
            video_log.points_earned = video_points_received
            user_data.add_points(video_points_received)

        db.put([user_video, user_data])

        # Defer the put of VideoLog for now, as we think it might be causing hot tablets
        # and want to shift it off to an automatically-retrying task queue.
        # http://ikaisays.com/2011/01/25/app-engine-datastore-tip-monotonically-increasing-values-are-bad/
        deferred.defer(commit_video_log, video_log, _queue="video-log-queue")

        return (user_video, video_log, video_points_total)

    def time_started(self):
        return self.time_watched - datetime.timedelta(seconds = self.seconds_watched)

    def time_ended(self):
        return self.time_watched

    def minutes_spent(self):
        return util.minutes_between(self.time_started(), self.time_ended())

    def key_for_video(self):
        return VideoLog.video.get_value_for_datastore(self)

# commit_video_log is used by our deferred video log insertion process
def commit_video_log(video_log):
    video_log.put()

class DailyActivityLog(db.Model):
    user = db.UserProperty()
    date = db.DateTimeProperty()
    activity_summary = object_property.ObjectProperty()

    @staticmethod
    def get_key_name(user_data, date):
        return "%s:%s" % (user_data.key_email, date.strftime("%Y-%m-%d-%H"))

    @staticmethod
    def build(user_data, date, activity_summary):
        log = DailyActivityLog(key_name=DailyActivityLog.get_key_name(user_data, date))
        log.user = user_data.user
        log.date = date
        log.activity_summary = activity_summary
        return log

    @staticmethod
    def get_for_user_data_between_dts(user_data, dt_a, dt_b):
        query = DailyActivityLog.all()
        query.filter('user =', user_data.user)

        query.filter('date >=', dt_a)
        query.filter('date <', dt_b)
        query.order('date')

        return query

class ProblemLog(db.Model):

    user = db.UserProperty()
    exercise = db.StringProperty()
    correct = db.BooleanProperty(default = False)
    time_done = db.DateTimeProperty(auto_now_add=True)
    time_taken = db.IntegerProperty(default = 0)
    problem_number = db.IntegerProperty(default = -1) # Used to reproduce problems
    exercise_non_summative = db.StringProperty() # Used to reproduce problems from summative exercises
    hint_used = db.BooleanProperty(default = False)
    points_earned = db.IntegerProperty(default = 0)
    earned_proficiency = db.BooleanProperty(default = False) # True if proficiency was earned on this problem
    sha1 = db.StringProperty()
    seed = db.StringProperty()
    count_attempts = db.IntegerProperty(default = 0)
    time_taken_attempts = db.ListProperty(int)
    attempts = db.StringListProperty()

    @property
    def ka_url(self):
        return absolute_url("/exercises?exid=%s&problem_number=%s" % \
            (self.exercise, self.problem_number))

    @staticmethod
    def get_for_user_data_between_dts(user_data, dt_a, dt_b):
        query = ProblemLog.all()
        query.filter('user =', user_data.user)

        query.filter('time_done >=', dt_a)
        query.filter('time_done <', dt_b)
        query.order('time_done')

        return query

    def time_taken_capped_for_reporting(self):
        # For reporting's sake, we cap the amount of time that you can be considered to be
        # working on a single problem at 60 minutes. If you've left your browser open
        # longer, you're probably not actively working on the problem.
        return min(consts.MAX_WORKING_ON_PROBLEM_SECONDS, self.time_taken)

    def time_started(self):
        return self.time_done - datetime.timedelta(seconds = self.time_taken_capped_for_reporting())

    def time_ended(self):
        return self.time_done

    def minutes_spent(self):
        return util.minutes_between(self.time_started(), self.time_ended())

# commit_problem_log is used by our deferred problem log insertion process
def commit_problem_log(problem_log_source):

    try:
        if not problem_log_source or not problem_log_source.key().name:
            return
    except db.NotSavedError:
        # Handle special case during new exercise deploy
        return

    def insert_in_position(index, items, val, filler):
        if index >= len(items):
            items.extend([filler] * (index + 1 - len(items)))
        items[index] = val

    # Committing transaction combines existing problem log with any followup attempts
    def txn():
        problem_log = ProblemLog.get_by_key_name(problem_log_source.key().name())

        if not problem_log:
            problem_log = ProblemLog(
                key_name = problem_log_source.key().name(),
                user = problem_log_source.user,
                exercise = problem_log_source.exercise,
                problem_number = problem_log_source.problem_number,
                time_done = problem_log_source.time_done,
                sha1 = problem_log_source.sha1,
                seed = problem_log_source.seed,
                exercise_non_summative = problem_log_source.exercise_non_summative,
        )

        index_attempt = max(0, problem_log_source.count_attempts - 1)
        if index_attempt < len(problem_log.time_taken_attempts) and problem_log.time_taken_attempts[index_attempt] != -1:
            # This attempt has already been logged. Ignore this dupe taskqueue execution.
            return

        # Bump up attempt count
        problem_log.count_attempts += 1

        # Hint used cannot be changed from True to False
        problem_log.hint_used = problem_log.hint_used or problem_log_source.hint_used

        # Correct cannot be changed from False to True after first attempt
        problem_log.correct = (problem_log_source.count_attempts == 1 or problem_log.correct) and problem_log_source.correct and not problem_log.hint_used

        # Add time_taken for this individual attempt
        problem_log.time_taken += problem_log_source.time_taken
        insert_in_position(index_attempt, problem_log.time_taken_attempts, problem_log_source.time_taken, filler=-1)

        # Add actual attempt content
        insert_in_position(index_attempt, problem_log.attempts, problem_log_source.attempts[0], filler="")

        # Points should only be earned once per problem, regardless of attempt count
        problem_log.points_earned = max(problem_log.points_earned, problem_log_source.points_earned)

        # Proficiency earned should never change per problem
        problem_log.earned_proficiency = problem_log.earned_proficiency or problem_log_source.earned_proficiency

        problem_log.put()

    db.run_in_transaction(txn)

# Represents a matching between a playlist and a video
# Allows us to keep track of which videos are in a playlist and
# which playlists a video belongs to (not 1-to-1 mapping)


class VideoPlaylist(db.Model):

    playlist = db.ReferenceProperty(Playlist)
    video = db.ReferenceProperty(Video)
    video_position = db.IntegerProperty()

    # Lets us enable/disable video playlist relationships in bulk without removing the entry
    live_association = db.BooleanProperty(default = False)
    last_live_association_generation = db.IntegerProperty(default = 0)

    _VIDEO_PLAYLIST_KEY_FORMAT = "VideoPlaylist_Videos_for_Playlist_%s"
    _PLAYLIST_VIDEO_KEY_FORMAT = "VideoPlaylist_Playlists_for_Video_%s"

    @staticmethod
    def get_cached_videos_for_playlist(playlist, limit=500):

        key = VideoPlaylist._VIDEO_PLAYLIST_KEY_FORMAT % playlist.key()
        namespace = str(App.version) + "_" + str(Setting.cached_library_content_date())

        videos = memcache.get(key, namespace=namespace)

        if not videos:
            videos = []
            query = VideoPlaylist.all()
            query.filter('playlist =', playlist)
            query.filter('live_association = ', True)
            query.order('video_position')
            video_playlists = query.fetch(limit)
            for video_playlist in video_playlists:
                videos.append(video_playlist.video)

            memcache.set(key, videos, namespace=namespace)

        return videos

    @staticmethod
    def get_cached_playlists_for_video(video, limit=5):

        key = VideoPlaylist._PLAYLIST_VIDEO_KEY_FORMAT % video.key()
        namespace = str(App.version) + "_" + str(Setting.cached_library_content_date())

        playlists = memcache.get(key, namespace=namespace)

        if playlists is None:
            playlists = []
            query = VideoPlaylist.all()
            query.filter('video =', video)
            query.filter('live_association = ', True)
            video_playlists = query.fetch(limit)
            for video_playlist in video_playlists:
                playlists.append(video_playlist.playlist)

            memcache.set(key, playlists, namespace=namespace)

        return playlists

    @staticmethod
    def get_query_for_playlist_title(playlist_title):
        query = Playlist.all()
        query.filter('title =', playlist_title)
        playlist = query.get()
        query = VideoPlaylist.all()
        query.filter('playlist =', playlist)
        query.filter('live_association = ', True) #need to change this to true once I'm done with all of my hacks
        query.order('video_position')
        return query

    @staticmethod
    def get_key_dict(query):
        video_playlist_key_dict = {}
        for video_playlist in query.fetch(10000):
            playlist_key = VideoPlaylist.playlist.get_value_for_datastore(video_playlist)

            if not video_playlist_key_dict.has_key(playlist_key):
                video_playlist_key_dict[playlist_key] = {}

            video_playlist_key_dict[playlist_key][VideoPlaylist.video.get_value_for_datastore(video_playlist)] = video_playlist

        return video_playlist_key_dict

class ExerciseVideo(db.Model):

    video = db.ReferenceProperty(Video)
    exercise = db.ReferenceProperty(Exercise)
    exercise_order = db.IntegerProperty()

    def key_for_video(self):
        return ExerciseVideo.video.get_value_for_datastore(self)

    @staticmethod
    def get_key_dict(query):
        exercise_video_key_dict = {}
        for exercise_video in query.fetch(10000):
            video_key = ExerciseVideo.video.get_value_for_datastore(exercise_video)

            if not exercise_video_key_dict.has_key(video_key):
                exercise_video_key_dict[video_key] = {}

            exercise_video_key_dict[video_key][ExerciseVideo.exercise.get_value_for_datastore(exercise_video)] = exercise_video

        return exercise_video_key_dict

class ExerciseGraph(object):

    def __init__(self, user_data):
        user_exercises = UserExercise.get_for_user_data_use_cache(user_data)
        exercises = Exercise.get_all_use_cache()
        self.exercises = exercises
        self.exercise_by_name = {}
        for ex in exercises:
            self.exercise_by_name[ex.name] = ex
            ex.coverers = []
            ex.user_exercise = None
            ex.next_review = None  # Not set initially
            ex.is_review_candidate = False
            ex.is_ancestor_review_candidate = None  # Not set initially
            ex.proficient = None # Not set initially
            ex.suggested = None # Not set initially
            ex.phantom = False
            ex.assigned = False
            ex.streak = 0
            ex.longest_streak = 0
            ex.total_done = 0
            if hasattr(ex, 'last_done'):
                # Clear leftovers from cache to fix random recents on new accounts
                del ex.last_done
        for name in user_data.proficient_exercises:
            ex = self.exercise_by_name.get(name)
            if ex:
                ex.proficient = True
        for name in user_data.assigned_exercises:
            ex = self.exercise_by_name.get(name)
            if ex:
                ex.assigned = True
        for ex in exercises:
            for covered in ex.covers:
                ex_cover = self.exercise_by_name.get(covered)
                if ex_cover:
                    ex_cover.coverers.append(ex)
            ex.prerequisites_ex = []
            for prereq in ex.prerequisites:
                ex_prereq = self.exercise_by_name.get(prereq)
                if ex_prereq:
                    ex.prerequisites_ex.append(ex_prereq)
        for user_ex in user_exercises:
            ex = self.exercise_by_name.get(user_ex.exercise)
            if ex and (not ex.user_exercise or ex.user_exercise.total_done < user_ex.total_done):
                ex.user_exercise = user_ex
                ex.streak = user_ex.streak
                ex.longest_streak = user_ex.longest_streak
                ex.total_done = user_ex.total_done
                ex.last_done = user_ex.last_done

        def compute_proficient(ex):
            # Consider an exercise proficient if it is explicitly proficient or
            # the user has never missed a problem and a covering ancestor is proficient
            if ex.proficient is not None:
                return ex.proficient
            ex.proficient = False
            if ex.streak == ex.total_done:
                for c in ex.coverers:
                    if compute_proficient(c) is True:
                        ex.proficient = True
                        break
            return ex.proficient

        for ex in exercises:
            compute_proficient(ex)

        def compute_suggested(ex):
            if ex.suggested is not None:
                return ex.suggested
            if ex.proficient is True:
                ex.suggested = False
                return ex.suggested
            ex.suggested = True
            # Don't suggest exs that are covered by suggested exs
            for c in ex.coverers:
                if compute_suggested(c) is True:
                    ex.suggested = False
                    return ex.suggested
            # Don't suggest exs if the user isn't proficient in all prereqs
            for prereq in ex.prerequisites_ex:
                if not prereq.proficient:
                    ex.suggested = False
                    break
            return ex.suggested

        for ex in exercises:
            compute_suggested(ex)
            ex.points = points.ExercisePointCalculator(ex, ex.suggested, ex.proficient)

        phantom = user_data.is_phantom
        for ex in exercises:
            ex.phantom = phantom


    def get_review_exercises(self, now = datetime.datetime.now()):

# An exercise ex should be reviewed iff all of the following are true:
#   * ex and all of ex's covering ancestors either
#      * are scheduled to have their next review in the past, or
#      * were answered incorrectly on last review (i.e. streak == 0 with proficient == True)
#   * None of ex's covering ancestors should be reviewed
#   * The user is proficient at ex
# The algorithm:
#   For each exercise:
#     traverse it's ancestors, computing and storing the next review time (if not already done),
#     using now as the next review time if proficient and streak==0
#   Select and mark the exercises in which the user is proficient but with next review times in the past as review candidates
#   For each of those candidates:
#     traverse it's ancestors, computing and storing whether an ancestor is also a candidate
#   All exercises that are candidates but do not have ancestors as candidates should be listed for review

        def compute_next_review(ex):
            if ex.next_review is None:
                ex.next_review = datetime.datetime.min
                if ex.user_exercise is not None and ex.user_exercise.last_review > datetime.datetime.min:
                    next_review = ex.user_exercise.last_review + ex.user_exercise.get_review_interval()
                    if next_review > now and ex.proficient and ex.user_exercise.streak == 0:
                        next_review = now
                    if next_review > ex.next_review:
                        ex.next_review = next_review
                for c in ex.coverers:
                    c_next_review = compute_next_review(c)
                    if c_next_review > ex.next_review:
                        ex.next_review = c_next_review
            return ex.next_review

        def compute_is_ancestor_review_candidate(rc):
            if rc.is_ancestor_review_candidate is None:
                rc.is_ancestor_review_candidate = False
                for c in rc.coverers:
                    rc.is_ancestor_review_candidate = rc.is_ancestor_review_candidate or c.is_review_candidate or compute_is_ancestor_review_candidate(c)
            return rc.is_ancestor_review_candidate

        for ex in self.exercises:
            compute_next_review(ex)
        review_candidates = []
        for ex in self.exercises:
            if not ex.summative and ex.proficient and ex.next_review <= now:
                ex.is_review_candidate = True
                review_candidates.append(ex)
            else:
                ex.is_review_candidate = False
        review_exercises = []
        for rc in review_candidates:
            if not compute_is_ancestor_review_candidate(rc):
                review_exercises.append(rc)
        return review_exercises

    def get_proficient_exercises(self):
        proficient_exercises = []
        for ex in self.exercises:
            if ex.proficient:
                proficient_exercises.append(ex)
        return proficient_exercises

    def get_summative_exercises(self):
        summative_exercises = []
        for ex in self.exercises:
            if ex.summative:
                summative_exercises.append(ex)
        return summative_exercises

    def get_suggested_exercises(self):
        # Mark an exercise as proficient if it or a a covering ancestor is proficient
        # Select all the exercises where the user is not proficient but the
        # user is proficient in all prereqs.
        suggested_exercises = []
        for ex in self.exercises:
            if ex.suggested:
                suggested_exercises.append(ex)
        return suggested_exercises

    def get_recent_exercises(self, n_recent=2):
        recent_exercises = sorted(self.exercises, reverse=True,
                key=lambda ex: ex.last_done if hasattr(ex, "last_done") and ex.last_done else datetime.datetime.min)

        recent_exercises = recent_exercises[0:n_recent]

        return filter(lambda ex: hasattr(ex, "last_done") and ex.last_done, recent_exercises)

from badges import util_badges, last_action_cache
from phantom_users import util_notify
